{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Softmax and multi-class classification\n",
    "\n",
    "- Multi-Class Classification (Softmax version): Similar to the Binary Classification Case, We have X (which are our predictors).  Each predictor describes the subject that we are trying to classify, and each row is an example.  The predictors are put into a linear combination of (wX+b) = y.\n",
    "\n",
    "- We need to create a probability space for the linear combination for each class. So take the exponent of linear combination results (y) for each class, and divide that by the exponent of total linear combination results.  The exponent is there to make sure all values are positive, so we donâ€™t get into a situation where we are dividing by zero.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "- NOTE: we need to One-Hot encode the y axis (similar to what we did to the perceptron algorithm, but in that case we used 1 and -1).  Why one-hot encode is because we don't want to imply any ordinal or distance dependencies between classes.  One-hot encoding just provides a numerical code to each group without the dependencies.\n",
    "\n",
    "- then continue on, and optimizing our predictions using the iterative process of gradient descent. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image('./images/one-hot-encoding.png')"
   ]
  },
  {
   "source": [
    "## Calculating the SoftMax Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from IPython import display\n",
    "\n",
    "pd.set_option('display.max_columns',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image('./images/softmax-function.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(scores:List[float])->List[float]:\n",
    "    exp_scores = np.exp(scores)\n",
    "    sum_exp_scores = sum(exp_scores)\n",
    "    result = []\n",
    "    for i in exp_scores:\n",
    "        result.append(i*1.0/sum_exp_scores)\n",
    "    return result\n",
    "    \n",
    "    # Note: The function np.divide can also be used here, as follows:\n",
    "    # def softmax(scores:List[float])->List[float]:\n",
    "    #     exp_scores = np.exp(scores)\n",
    "    #     return np.divide (exp_scores,sum_exp_scores)"
   ]
  }
 ]
}