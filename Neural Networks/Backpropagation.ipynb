{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Back Propagation\n",
    "\n",
    "Now we've come to the problem of how to make a multilayer neural network learn. Before, we saw how to update weights with gradient descent. The backpropagation algorithm is just an extension of that, using the chain rule to find the error with the respect to the weights connecting the input layer to the hidden layer (for a two layer network).\n",
    "\n",
    "To update the weights to hidden layers using gradient descent, you need to know how much error each of the hidden units contributed to the final output. Since the output of a layer is determined by the weights between layers, the error resulting from units is scaled by the weights going forward through the network. Since we know the error at the output, we can use the weights to work backwards to hidden layers.\n",
    "\n",
    "For example, in the output layer, you have errors $\\delta^o_k$ attributed to each output unit $k$. Then the error attributed to hidden unit $j$ is the output errors, scaled by the weights between the output and hidden layers (and the gradient):\n",
    "\n",
    "\n",
    "![Output Error](./images/backprop-error.gif)\n",
    "\n",
    "\n",
    "Then, the gradient descent step is the same as before, just with the new errors:\n",
    "\n",
    "\n",
    "![Error Update](./images/backprop-weight-update.gif)\n",
    "\n",
    "\n",
    "where $w_{ij}$ are the weights between the inputs and hidden layer and $x_i$ are input unit values. This form holds for however many layers there are. The weight steps are equal to the step size times the output error of the layer times the values of the inputs to that layer\n",
    "\n",
    "\n",
    "![general-backprop](./images/backprop-general.gif)\n",
    "\n",
    "\n",
    "Here, you get the output error, $\\delta_{output}$, by propagating the errors backwards from higher layers. And the input values, $V_{in}$ are the inputs to the layer, the hidden layer activations to the output unit for example."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}